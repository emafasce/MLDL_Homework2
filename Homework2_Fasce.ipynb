{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework2_Fasce.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9QcGnGPdX2C"
      },
      "source": [
        "\n",
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9O3aM3Tb28q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        },
        "outputId": "164d1bc1-7920-407e-a36e-dd18c6b03f1f"
      },
      "source": [
        "!python -m pip install -U pip\n",
        "!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/eb/4a3642e971f404d69d4f6fa3885559d67562801b99d7592487f1ecc4e017/pip-20.3.3-py2.py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 4.0MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-20.3.3\n",
            "Collecting torch==1.3.1\n",
            "  Downloading torch-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (734.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 734.6 MB 21 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.3.1) (1.19.4)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.8.1+cu101 requires torch==1.7.0, but you have torch 1.3.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.3.1\n",
            "Collecting torchvision==0.5.0\n",
            "  Downloading torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.15.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.19.4)\n",
            "Collecting torch==1.4.0\n",
            "  Downloading torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 19 kB/s \n",
            "\u001b[?25hInstalling collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.3.1\n",
            "    Uninstalling torch-1.3.1:\n",
            "      Successfully uninstalled torch-1.3.1\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.8.1+cu101\n",
            "    Uninstalling torchvision-0.8.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.8.1+cu101\n",
            "Successfully installed torch-1.4.0 torchvision-0.5.0\n",
            "Collecting Pillow-SIMD\n",
            "  Downloading Pillow-SIMD-7.0.0.post3.tar.gz (630 kB)\n",
            "\u001b[K     |████████████████████████████████| 630 kB 5.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Pillow-SIMD\n",
            "  Building wheel for Pillow-SIMD (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pillow-SIMD: filename=Pillow_SIMD-7.0.0.post3-cp36-cp36m-linux_x86_64.whl size=1110312 sha256=cf73359ef57d53d57c5c590ea19f1afad1bb9779ffe39a137dd278af89613baf\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/a4/d6/6a2877ff61c41f6cbeced16ca150601820ed3f8b27cb7faf8d\n",
            "Successfully built Pillow-SIMD\n",
            "Installing collected packages: Pillow-SIMD\n",
            "Successfully installed Pillow-SIMD-7.0.0.post3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pVJqPPlEfKk"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jqEa0JKOHgo"
      },
      "source": [
        "import os\r\n",
        "import logging\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.utils.data import Subset, DataLoader\r\n",
        "from torch.backends import cudnn\r\n",
        "import torchvision\r\n",
        "from torchvision import transforms\r\n",
        "from PIL import Image\r\n",
        "from tqdm import tqdm\r\n",
        "from torchvision.datasets import VisionDataset\r\n",
        "from PIL import Image\r\n",
        "import os\r\n",
        "import os.path\r\n",
        "import sys\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from torchvision.models import alexnet\r\n",
        "import numpy as np\r\n",
        "from PIL import ImageDraw\r\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1Jjrpg1E4J7"
      },
      "source": [
        "**Function used to plot the loss and accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R33OKYqYExeJ"
      },
      "source": [
        "def plotting(loss_train, loss_val, acc_train, acc_val):\r\n",
        "  \r\n",
        "  epochs=len(loss_train)\r\n",
        "  x=[i+1 for i in range(epochs)]\r\n",
        "  x_div=int(epochs/6)\r\n",
        "  xdisp=[i for i in range(epochs) if i%x_div==1]\r\n",
        "  plt.figure()\r\n",
        "  plt.plot(x, loss_train, 'r', label='Training loss')\r\n",
        "  plt.plot(x, loss_val, 'b', label='Validation loss')\r\n",
        "  plt.legend()\r\n",
        "  plt.xlabel('Epochs')\r\n",
        "  plt.xticks(xdisp)\r\n",
        "  plt.savefig('Losses', format='png')\r\n",
        "  plt.figure()\r\n",
        "  plt.plot(x, acc_train, 'r', label='Training accuracy')\r\n",
        "  plt.plot(x, acc_val, 'b', label='Validation accuracy')\r\n",
        "  plt.legend()\r\n",
        "  plt.xticks(xdisp)\r\n",
        "  plt.xlabel('Epochs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jMupym_LR1j"
      },
      "source": [
        "**Functions implementing the logic to load the database**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zTFFMm2IXYu"
      },
      "source": [
        "def pil_loader(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n",
        "\n",
        "class Caltech(VisionDataset):\n",
        "\n",
        "    def __init__(self, root, split, transform=None, transform_train=None, transform_val=None, train_or_val=None):\n",
        "\n",
        "    #In this implementation the first row of the training file contains a picture of the training set, and the second row a picture of the validation set.\n",
        "    #Since pictures of the same classes are in sequential rows, this ensures the same number of pictures for each class in the training and validation set.\n",
        "    #I used it for the 4th section of the homework since I find this logic of implementation a bit easier when exploiting the data augmentation technique.  \n",
        "    #However, in the comments another implementation is introduced, which uses the standard train test split function with stratify.\n",
        "\n",
        "        self.images =[]\n",
        "        self.images_val=[]\n",
        "        self.indexes = {}\n",
        "        super(Caltech, self).__init__(root, transform=transform)\n",
        "        self.split = split\n",
        "        self.train_or_val=train_or_val\n",
        "        self.transform=transform\n",
        "        self.transform_train=transform_train\n",
        "        self.transform_val=transform_val\n",
        "        skip = False\n",
        "        filenames = open(root.split(\"/\")[0]+\"/\"+split+\".txt\", \"r\")\n",
        "        i=0\n",
        "        ind='TRAIN'\n",
        "        for filename in filenames.readlines():\n",
        "            skip=False\n",
        "            for class_ignored in  [\"BACKGROUND_Google\"]:\n",
        "                if filename.startswith(class_ignored):\n",
        "                    skip=True\n",
        "                    break\n",
        "            if skip:\n",
        "                continue\n",
        "            label = filename.split(\"/\")[0]\n",
        "            if self.split=='train':\n",
        "              if ind=='TRAIN':\n",
        "                self.images.append((pil_loader(root+\"/\"+filename.rstrip()), label))\n",
        "              else:\n",
        "                self.images_val.append((pil_loader(root+\"/\"+filename.rstrip()), label))\n",
        "            else:\n",
        "              self.images.append((pil_loader(root+\"/\"+filename.rstrip()), label))\n",
        "            if label not in self.indexes.keys():\n",
        "                self.indexes[label]=i\n",
        "                i+=1\n",
        "            if self.split=='train':\n",
        "              if ind=='TRAIN':\n",
        "                ind='VAL'\n",
        "              else:\n",
        "                ind='TRAIN'\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        train_or_val=self.train_or_val\n",
        "        if train_or_val=='train':\n",
        "            image, label = self.images[index][0], self.indexes[self.images[index][1]]\n",
        "            image=self.transform_train(image)\n",
        "            return image, label\n",
        "        if train_or_val=='val':\n",
        "            image, label = self.images_val[index][0], self.indexes[self.images[index][1]]\n",
        "            image=self.transform_val(image)\n",
        "            return image, label\n",
        "        else:\n",
        "            image, label = self.images[index][0], self.indexes[self.images[index][1]]\n",
        "            image=self.transform(image)\n",
        "            return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        if self.train_or_val=='val':\n",
        "            return len(self.images_val)\n",
        "        else:\n",
        "            return len(self.images)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJbjLU56GLlB"
      },
      "source": [
        "**Function used to load the database**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDLJuIXK_vh"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5PkYfqfK_SA"
      },
      "source": [
        "def init(pre_trained, batch_size):\n",
        "\n",
        "  if pre_trained==True:\n",
        "\n",
        "    train_transform = transforms.Compose([transforms.Resize(256),    \n",
        "                                        transforms.CenterCrop(224),  \n",
        "                                        #transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                        #transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
        "                                        #transforms.RandomCrop(size=130),                                                        \n",
        "                                        transforms.ToTensor(), \n",
        "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
        "    ])\n",
        "    val_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                        transforms.CenterCrop(224),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])  \n",
        "    test_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                        transforms.CenterCrop(224),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])   \n",
        "                                    \n",
        "  if pre_trained==False:\n",
        "\n",
        "    train_transform = transforms.Compose([transforms.Resize(256),     \n",
        "                                        transforms.CenterCrop(224),                                                                \n",
        "                                        #transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                        transforms.ToTensor(), \n",
        "                                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "    ])\n",
        "    val_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                        transforms.CenterCrop(224),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                         ])\n",
        "    test_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                        transforms.CenterCrop(224),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                         ])\n",
        "\n",
        "  if not os.path.isdir('./Caltech101'):\n",
        "    !git clone https://github.com/MachineLearning2020/Homework2-Caltech101.git\n",
        "    !mv 'Homework2-Caltech101' 'Caltech101'\n",
        "  DATA_DIR = 'Caltech101/101_ObjectCategories'\n",
        "\n",
        "  train_dataset = Caltech(DATA_DIR, split='train', train_or_val='train', transform_train=train_transform)\n",
        "  val_dataset= Caltech(DATA_DIR, split='train', train_or_val='val', transform_val=val_transform)\n",
        "  test_dataset = Caltech(DATA_DIR, split='test', transform=test_transform)\n",
        "  \n",
        "  '''\n",
        "  labels=[]\n",
        "  for i in range(len(train_dataset)):\n",
        "    labels.append(train_dataset[i][1])\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  train, val=train_test_split(train_dataset, stratify=labels, test_size=0.5, random_state=42)\n",
        "  '''\n",
        "\n",
        "  print('Train Dataset: {}'.format(len(train_dataset)))\n",
        "  print('Valid Dataset: {}'.format(len(val_dataset)))\n",
        "  print('Test Dataset: {}'.format(len(test_dataset)))\n",
        "\n",
        "  # Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
        "  val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)  \n",
        "\n",
        "  return train_dataloader, val_dataloader, test_dataloader, train_dataset, val_dataset, test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qYIHPzYLY7i"
      },
      "source": [
        "**Training, validation and test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfVq_uDHLbsR"
      },
      "source": [
        "def train_net(net, layer, train_dataloader,  val_dataloader, test_dataloader, train_dataset, val_dataset, test_dataset, NUM_CLASSES, BATCH_SIZE,LR, MOMENTUM, WEIGHT_DECAY, NUM_EPOCHS, STEP_SIZE, GAMMA, LOG_FREQUENCY):\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss() \n",
        "\n",
        "  if layer=='all':\n",
        "    parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet\n",
        "  if layer=='fully':\n",
        "    parameters_to_optimize=net.classifier.parameters()\n",
        "  if layer=='conv':\n",
        "    parameters_to_optimize=net.features.parameters()\n",
        "\n",
        "  optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "  #optimizer=optim.Adadelta(parameters_to_optimize, lr=LR)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "  print('preparing validation!')\n",
        "\n",
        "  cudnn.benchmark \n",
        "\n",
        "  current_step = 0\n",
        "  val_acc_list=[]\n",
        "  val_loss_list=[]\n",
        "  train_loss_list=[]\n",
        "  train_acc_list=[]\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "    net = net.to(DEVICE)\n",
        "    net.eval()\n",
        "    running_corrects = 0\n",
        "    train_loss=0\n",
        "\n",
        "    #Validation is made before training in order to check the accuracy of pretrained models\n",
        "\n",
        "    for images, labels in (val_dataloader):\n",
        "\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "      outputs = net(images)\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "      local_loss = criterion(outputs, labels)\n",
        "      train_loss+=local_loss.item()\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "    train_loss=train_loss/len(val_dataloader)\n",
        "    print('Validation Loss: {}'.format(train_loss))\n",
        "    accuracy = running_corrects / float(len(val_dataset))\n",
        "    print('Validation Accuracy: {}'.format(accuracy))\n",
        "    val_acc_list.append(accuracy)\n",
        "    val_loss_list.append(train_loss)\n",
        "\n",
        "    running_corrects = 0\n",
        "    train_loss=0\n",
        "\n",
        "    for images, labels in train_dataloader:     \n",
        "\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "      net.train() \n",
        "      optimizer.zero_grad() \n",
        "      outputs = net(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      train_loss+=loss.item()\n",
        "      loss.backward()\n",
        "      optimizer.step() \n",
        "      current_step += 1\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "    train_loss=train_loss/len(train_dataloader)\n",
        "    print('Training Loss: {}'.format(train_loss))\n",
        "    accuracy = running_corrects / float(len(train_dataset))\n",
        "    print('Training Accuracy: {}'.format(accuracy))\n",
        "    train_acc_list.append(accuracy)\n",
        "    train_loss_list.append(train_loss)\n",
        "    scheduler.step()\n",
        "\n",
        "  #Testing\n",
        "\n",
        "  net = net.to(DEVICE) \n",
        "  net.eval() \n",
        "  running_corrects = 0\n",
        "\n",
        "  for images, labels in tqdm(test_dataloader):\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "    outputs = net(images)\n",
        "    _, preds = torch.max(outputs.data, 1)\n",
        "    running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "  accuracy = running_corrects / float(len(test_dataset))\n",
        "  print('Test Accuracy: {}'.format(accuracy))\n",
        "\n",
        "  return train_loss_list, train_acc_list, val_loss_list, val_acc_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-eNn36zL28k"
      },
      "source": [
        "**Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rWEkOt8VVLr"
      },
      "source": [
        "DEVICE = 'cuda'\r\n",
        "NUM_CLASSES = 101\r\n",
        "BATCH_SIZE = 10\r\n",
        "LR = 1e-3\r\n",
        "MOMENTUM = 0.9\r\n",
        "WEIGHT_DECAY = 5e-5\r\n",
        "NUM_EPOCHS = 50\r\n",
        "STEP_SIZE = 30\r\n",
        "GAMMA = 0.1\r\n",
        "LOG_FREQUENCY = 10\r\n",
        "pre_trained=True\r\n",
        "parameters='conv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfJm9CrSVvLv"
      },
      "source": [
        "**Loading, training, evaluating, testing the network and plotting the loss and the accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3bbhRRMd_5O"
      },
      "source": [
        "train_dataloader, val_dataloader, test_dataloader,train,val,test=init(pre_trained, BATCH_SIZE)\n",
        "\n",
        "import torchvision.models as models\n",
        "net=models.alexnet(pretrained=True).to(DEVICE)\n",
        "net.classifier[6]=nn.Linear(4096, 101)\n",
        "\n",
        "train_loss_list, train_acc_list, val_loss_list, val_acc_list=train_net(net, parameters, train_dataloader, val_dataloader, test_dataloader, train, val, test, NUM_CLASSES, BATCH_SIZE,LR, MOMENTUM, WEIGHT_DECAY, NUM_EPOCHS, STEP_SIZE, GAMMA, LOG_FREQUENCY)\n",
        "plotting(train_loss_list, val_loss_list, train_acc_list, val_acc_list)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
